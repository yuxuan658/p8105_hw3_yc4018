---
title: "p8105_hw3_yc4018"
author: "Yuxuan Chen"
output: github_document
---
 - Code for setting options of graphs. 
```{r message = FALSE}
library(tidyverse)
library(patchwork)
library(p8105.datasets)

theme_set(theme_minimal() + theme(legend.position = "right"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

**Problem 1**   
load the `instacart` data.
```{r collapse = TRUE}
data("instacart")
```

1. How many aisles are there, and which aisles are the most items ordered from?
```{r}
aisle_obs = 
  instacart %>%
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs))

knitr::kable(aisle_obs[0:10,])
```
 - There are `r nrow(aisle_obs)` aisles in the dataset. Since `r aisle_obs[1,2]` is the largest observations of aisle, fresh vegetables is the aisle that the most items ordered from.

2. Below is a scatter plot that shows the number of items ordered in each aisle, which with more than 10000 items ordered.    
```{r}
aisle_obs %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n_obs)) %>% 
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(
    title = "The Number of Items Ordered In Each Aisle",
    x = "Aisle",
    y = "Number of items"
  )
```

 - Based on the above scatter plot, we can see that butter is the aisle that has the lowest number of items ordered, and fresh vegetables is the aisle that the most items ordered from.

3. Below table shows the 3 most popular products in each of the aisles of “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, with the number of times each item is ordered.

```{r message = FALSE}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_times = n()) %>% 
  mutate(
    product_rank = min_rank(desc(n_times)) #rank the items from most popular to least popular
  ) %>% 
  filter(product_rank <= 3) %>% 
  arrange(aisle, product_rank) %>% 
  knitr::kable()
```

 - Based on the table, we can see that for the aisle of “baking ingredients”, the 3 most popular products are Light Brown Sugar, Pure Baking Soda, and Cane Sugar. For the aisle of “dog food care”, the 3 most popular products are Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken & Brown Rice Recipe, and Small Dog Biscuits. And for the aisle of “packaged vegetables fruits”, the 3 most popular products are Organic Baby Spinach, Organic Raspberries, and Organic Blueberries
 
4. Below table shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r message = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  mutate(
    order_dow = lubridate::wday(order_dow + 1, label = TRUE, abbr = FALSE)) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>% 
  knitr::kable(digits = 2)
```

5. Dataset Interpretation:
 - The instacart dataset contains 1384617 observations of 15 variables, and each row in the dataset is a product from an order, which helps to examine the trends in online grocery purchasing. The size of this dataset is (`r dim(instacart)`). I think the key variables are `aisle`; `department`; `product_name`; `user_id`; `add_to_cart_order`, which represents the order in which each product was added to cart; `reordered`, which shows if this product has been ordered by this user in the past; `order_dow`, which shows the day of the week on which the order was placed; `order_hour_of_day`, which represents the hour of the day on which the order was placed; and `days_since_prior_order`, which indicates the days since the last order. For example, the customer with a user id of 112108 ordered Bulgarian Yogurt 9 days after he/she prior order. 

***
**Problem 2**   
load, clean, and tidy the BRFSS data.
```{r warning = FALSE}
data("brfss_smart2010")

brfss_df = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% #use appropriate variable names
  filter(topic == "Overall Health") %>%  # focus on Overall Health topic
  separate(locationdesc, into = c("state", "location"), " - ", convert = TRUE) %>%
  select(-locationabbr) %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))

knitr::kable(brfss_df[0:4,])
```

1. In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r message = FALSE, collapse = TRUE}
#In 2002
brfss_2002 = 
  brfss_df %>% 
  filter(year == 2002) %>%
  select(state, location) %>%
  group_by(state) %>% 
  distinct(location, .keep_all = TRUE) %>%
  summarize(n_location = n()) %>% 
  filter(n_location >= 7) 
knitr::kable(brfss_2002)

#in 2010
brfss_2010 = 
  brfss_df %>% 
  filter(year == 2010) %>%
  select(state, location) %>%
  group_by(state) %>% 
  distinct(location, .keep_all = TRUE) %>%
  summarize(n_location = n()) %>% 
  filter(n_location >= 7) 
knitr::kable(brfss_2010)
```

 - The above datasets show the states that were observed at 7 or more locations in 2002 and 2010. Hence, in 2002, CT, FL, MA, NC, NJ, PA are the states that were observed at 7 or more locations. And in 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA are the states that were observed at 7 or more locations.

2. 1)Below is the dataset that is limited to `Excellent` responses, and contains year, state, and the averages of the `data_value` across locations within a state.
```{r message = FALSE}
brfss_excellent = 
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(
    mean = mean(data_value, na.rm = TRUE)
  )  
knitr::kable(brfss_excellent[0:5,]) 
```

2)Below is the “spaghetti” plot of the averages of the `data_value` over time within a state. 
```{r}
brfss_excellent %>% 
  ggplot(aes(x = year, y = mean, color = state)) + 
  geom_point(alpha = .5) + 
  geom_line(alpha = .5) +
  labs(
    title = "Average Data Value Over Time Within A State",
    x = "Year",
    y = "Average Data Value",
    caption = "Data from BRFSS dataset for 2002-2010"
  ) 
```

3. Below is the two-panel plot showing the distribution of data value for responses (“Poor” to “Excellent”) among locations in NY State for the years 2006 and 2010.
```{r}
brfss_df %>% 
  filter(
    year == 2006 | year == 2010,
    state == "NY"
  ) %>% 
  ggplot(aes(x = response, y = data_value, fill = response)) + 
  geom_boxplot() +
  facet_grid(. ~ year) + 
  labs(
    title = "Distribution of Data Value for Responses Among Locations in NY State",
    x = "Level of Response",
    y = "Data Value",
    caption = "Data from BRFSS dataset for the years 2006 and 2010"
  ) 
```

 - Based on the above boxplot, we can see that in both 2006 and 2010, the majority level of response from people in NY State are "Good" and "Very good", and only a few people's responses are at the level of "Poor". From 2006 to 2010, the mean or the range of data value in each level of response has increased, for example, more people in NY State respond to the question at the level of "Very good".


4. Dataset Interpretation:
 - After loading, cleaning, and tidying the BRFSS data, I have a resulting dataset named `brfss_df` dataset. The `brfss_df` dataset is from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010, which contains 10625 observations of 23 variables. The dimension of this dataset is (`r dim(brfss_df)`). I think the key variables are: year, state, location, class, topic, question, and response. 

***

**Problem 3**

1. Load and tidy the accelerometer data. 
```{r message = FALSE}
accel_df = read_csv("./data/accel_data.csv")

accel_df = 
  accel_df %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_in_a_day",
    names_prefix = "activity.", #exclude the `activity.` prefix in each case
    values_to = "activity_count"
  ) %>% 
  mutate(
    type_of_day = if_else( day %in% c("Saturday", "Sunday" ), "weekend", "weekday"),
    minute_in_a_day = as.numeric(minute_in_a_day),
    ####day_id = as.numeric(day_id),
    day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
    ) %>% 
  relocate(week, day_id, day, type_of_day)

knitr::kable(accel_df[0:4,])
```

 - After loading, cleaning, and tidying the accelerometer data, I have a resulting dataset named `accel_df` dataset. The `accel_df` dataset is a collected accelerometer data from a 63-year-old male with BMI 25 for five weeks, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). This dataset contains 50400 observations with 6 variables. The dimension of this dataset is (`r dim(accel_df)`). The variables included in this dataset are `week`, `day_id`, `day`, `minute_in_a_day`, `activity_count`, and `type_of_day`. The variable `week` shows the number of the week; variable `day_id` indicates the unique number of each day; variable `day` indicates the day of the week; variable `minute_in_a_day` indicates the number of minute of a 24-hour day starting at midnight; variable `activity_count` indicates activity counts for each minute; and variable `type_of_day` indicates this day is a weekday or weekend. 

2. Below is the table of the total activity count for each day.
```{r message = FALSE}
total_activity_df = 
  accel_df %>% 
  group_by(week, day_id, day) %>% 
  summarize(
    total_activity_count = sum(activity_count)
  ) 
knitr::kable(total_activity_df)

gg_day_activity = 
  total_activity_df %>% 
  ggplot(aes(x = day, y = total_activity_count, group = week, color = week)) + 
  geom_point(alpha = .5) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(
    #title = "Distribution of Total Activity Count in Each Day Per Week",
    x = "Day",
    y = "Total Activity Count in Each Day",
  ) 

gg_dayid_activity = 
  total_activity_df %>% 
  ggplot(aes(x = day_id, y = total_activity_count, color = week)) + 
  geom_point() +
  geom_line() +
  labs(
    #title = "Distribution of Total Activity Count in Each Day",
    x = "Day id",
    y = "Total Activity Count in Each Day",
  ) 

(gg_dayid_activity + gg_day_activity) + 
  plot_annotation(title = "Distribution of Total Activity Count") & 
  theme(plot.title = element_text(hjust = 0.5))
```

 - Since traditional analyses of accelerometer data focus on the total activity over the day, I drew two plots above. The left graph simply shows the distribution of the total activity count in each day. On the right graph I grouped the day by week, and the plot shows the distribution of total activity count in each day based on the week for 5 weeks. 
 - Based on the two plots above, the total activity count fluctuates and oscillates up and down, but roughly spread around 3.5*10^5 counts for each day. In the first 2 weeks, the total activity count on Monday is lower than that on Friday; but in the rest of three weeks, the total activity count on Monday has a larger value than on Friday. The total activity count always increases from Tuesday to Wednesday. Saturday has a high total activity count for the first three weeks but becomes 0 in the last two weeks.    
 
3. Below is a single-panel plot that shows the 24-hour activity time courses for each day for 5 weeks. 
```{r message = FALSE}
accel_df %>% 
  ggplot(aes(x = minute_in_a_day, y = activity_count, group = day_id, color = day)) +
  geom_line(alpha = .5) +
  geom_smooth(se = FALSE, aes(group = day)) +
  labs(
    title = "24-hour Activity Time Courses For Each Day For 5 Weeks",
    x = "Minute of The Day",
    y = "Activity Count",
  ) 
```

 - Since accelerometer data allows the inspection activity over the course of the day. Based on the above plot of 24-hour activity time courses for each day for 5 weeks, we can see that the activity counts are small in the early morning; around the 250 minutes of the day, the activity counts start increasing to around 2400 and stay for the rest of the day; on the evening, activity count has a rapid growth which is starting from the 1200 minute of the day, then becomes decreasing at around 1300 minute of the day. Saturday has a high activity count in the early morning. During the week, the activity count has the highest value in the evening of most of the days. 
